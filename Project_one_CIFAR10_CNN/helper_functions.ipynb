{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7ed0f90-afd7-4c69-bbc6-95fa4d816deb",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15759947-e57e-4856-b1c3-d4e7e18a364f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9148d16-87be-4101-afcc-114cca08ef19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the seed\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5219a2b6-4e88-40c4-8f73-36a6b9a11ebb",
   "metadata": {},
   "source": [
    "## Image Visualization Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbbf190e-60ee-4da0-a2a8-67966fbf64ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def view_random_images(X, y, classes_names, seed=42):\n",
    "    \"\"\"\n",
    "    Displays a grid of 1 sample image per class from the dataset, along with their corresponding labels.\n",
    "\n",
    "    Args:\n",
    "    \n",
    "        X: array-like\n",
    "            The image dataset, typically a 4D array with shape (num_samples, height, width, channels).\n",
    "        y: array-like\n",
    "            The labels corresponding to each image in `X`, typically a 1D array with shape (num_samples,).\n",
    "        classes_names: list of str\n",
    "            A list of class names where each index corresponds to a class label in `y`.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "            Displays images in a grid with their respective labels beneath each image.\n",
    "\n",
    "    Notes:\n",
    "        - This function assumes that `y` contains integer class labels and that each class has at least one image.\n",
    "        - Images are displayed in a grid format, with each label shown below its corresponding image.\n",
    "    \"\"\"\n",
    "    \n",
    "    labels = np.unique(y)\n",
    "    num_classes = len(labels)\n",
    "    num_cols = 5  # Set a fixed number of columns\n",
    "    num_rows = (num_classes + num_cols - 1) // num_cols  # Calculate rows to fit all classes\n",
    "\n",
    "    plt.figure(figsize=(16, 10))\n",
    "    for i, label in enumerate(labels):\n",
    "        label_indices = [index for index, yi in enumerate(y) if yi == label]\n",
    "        rand_ind = random.choice(label_indices)  # Select a random index for the current label\n",
    "        \n",
    "        plt.subplot(num_rows, num_cols, i + 1)\n",
    "        plt.imshow(X[rand_ind])\n",
    "        plt.title(classes_names[label])\n",
    "        plt.axis('off')  # Hide axes for clarity\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e08355-2fab-45bf-9941-3b704bc638d1",
   "metadata": {},
   "source": [
    "## Plot loss and accuracy curves function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb72aaa5-008b-4d19-b8d6-87cc325976bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_curves(model_history):\n",
    "    \"\"\"\n",
    "    This function generates two plots:\n",
    "    1. Training Loss vs. Validation Loss.\n",
    "    2. Training Accuracy vs. Validation Accuracy.\n",
    "\n",
    "    Args:\n",
    "        model_history (History): A Keras History object containing the training and validation metrics over epochs.\n",
    "\n",
    "    Returns:\n",
    "        None: The function plots the curves and does not return any value.\n",
    "    \"\"\"\n",
    "    # Losses\n",
    "    training_loss = model_history.history[\"loss\"]  # training loss across epochs\n",
    "    val_loss = model_history.history[\"val_loss\"]  # validation loss across epochs\n",
    "\n",
    "    # Accuracies\n",
    "    training_accuracy = model_history.history[\"accuracy\"]  # training accuracy across epochs\n",
    "    val_accuracy = model_history.history[\"val_accuracy\"]  # validation accuracy across epochs\n",
    "\n",
    "    number_of_epochs = len(training_loss)\n",
    "    epochs = range(number_of_epochs)\n",
    "\n",
    "    # Plot losses\n",
    "    plt.plot(epochs, training_loss, label='training_loss')\n",
    "    plt.plot(epochs, val_loss, label='validation_loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.title('Training loss vs. Validation loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot accuracies\n",
    "    plt.figure()  # a new figure for a separate plot\n",
    "    plt.plot(epochs, training_accuracy, label='training_accuracy')\n",
    "    plt.plot(epochs, val_accuracy, label='validation_accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.title('Training accuracy vs. validation accuracy')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b56e1a-6e83-4307-89d9-cfd90532f8d9",
   "metadata": {},
   "source": [
    "## Model Evaluation results (for classification tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df5e3ef2-db40-4af0-87e1-f6bb8d513c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "def model_evaluation(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Performs model evaluation by calculating model accuracy, precision, recall and f1 score for a multi-class classification model.\n",
    "\n",
    "    Args:\n",
    "        y_true (1D array): true labels (ground truth).\n",
    "        y_pred (1D array): predicted labels.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary of accuracy, precision, recall, and f1-score.\n",
    "    \"\"\"\n",
    "\n",
    "    # calculate accuracy\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "    # calculate precision, recall and f1-score\n",
    "    precision, recall, f1_score, _ = precision_recall_fscore_support(y_true, y_pred, average=\"weighted\")\n",
    "\n",
    "    # reuslts in a dictionary\n",
    "    model_results = {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1-score\": f1_score,\n",
    "    }\n",
    "    \n",
    "    return model_results\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479488ac-d9cf-4385-b5b2-09c5e30d5035",
   "metadata": {},
   "source": [
    "## LearningRateScheduler function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "926c0810-fe96-4eb6-9927-576e29b18e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_schedule(epoch_number, initial_lr=1e-3, drop_rate=0.5, epochs_drop=10):\n",
    "    \"\"\"\n",
    "    Adjust the learning rate based on the current epoch.\n",
    "    Note: This function is needed for the 'LearningRateScheduler' tf.keras callback\n",
    "\n",
    "    Parameters:\n",
    "        epoch_number (int): Current epoch number\n",
    "        initial_lr (float): Initial learning rate (default 1e-3)\n",
    "        drop_rate (float): Factor to drop the learning rate (default 0.5)\n",
    "        epochs_drop (int): Number of epochs before dropping the learning rate (default 10)\n",
    "\n",
    "    Returns:\n",
    "        lr (float): Updated learning rate\n",
    "    \"\"\"\n",
    "    # validate inputs\n",
    "    if not (0 < drop_rate <= 1):\n",
    "        raise ValueError(\"drop_rate must be between 0 and 1\")\n",
    "    if epochs_drop <= 0:\n",
    "        raise ValueError(\"epochs_drop must be a positive integer\")\n",
    "\n",
    "    # compute learning rate\n",
    "    lr = initial_lr * (drop_rate ** (epoch_number // epochs_drop))\n",
    "    \n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e8904a-db0a-4018-b63e-3d22e5c9d85b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28357e4d-b7f6-43b7-a37d-4b13e93e6f55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "60ba01b6-ae89-40b7-931a-8cb434b6d344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook helper_functions.ipynb to script\n",
      "[NbConvertApp] Writing 5525 bytes to helper_functions.py\n"
     ]
    }
   ],
   "source": [
    "# !jupyter nbconvert --to script helper_functions.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfa449e-cc21-4bd2-b8b9-abed59fd8a1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58e5ed9-05a7-43ad-aba1-e7d4e15eea4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_env)",
   "language": "python",
   "name": "tf_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
